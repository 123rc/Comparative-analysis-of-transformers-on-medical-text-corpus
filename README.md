# Comparative-analysis-of-transformers-on-medical-text-corpus

Explored a variety of transformer-based architectures, including BERT, GPT, and RoBERTa, to capture semantic relationships in textual data effectively.
Designed and implemented a hybrid model that combines the strengths of BERT's contextual embeddings with the sequential modeling capabilities of LSTM for a 15% improvement in text classification performance.
